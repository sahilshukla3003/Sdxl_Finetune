2025-11-19 11:55:52 - INFO - Configuration saved
2025-11-19 11:55:52 - INFO - Accelerator device: cuda
2025-11-19 11:55:52 - INFO - Mixed precision: fp16
2025-11-19 11:55:52 - INFO - Num processes: 1
2025-11-19 11:55:52 - INFO - Loading tokenizers...
2025-11-19 11:55:53 - INFO - Loading noise scheduler...
2025-11-19 11:55:53 - INFO - Loading VAE...
2025-11-19 11:55:53 - INFO - ✓ VAE loaded
2025-11-19 11:55:53 - INFO - Loading text encoders...
2025-11-19 11:55:55 - INFO - ✓ Text encoders loaded
2025-11-19 11:55:55 - INFO - Loading UNet...
2025-11-19 11:55:56 - INFO - ✓ Gradient checkpointing enabled
2025-11-19 11:55:56 - INFO - ✓ UNet loaded
2025-11-19 11:55:56 - INFO - Applying LoRA (rank=16)...
2025-11-19 11:55:57 - INFO - ✓ LoRA applied
2025-11-19 11:55:57 - INFO - Loading dataset: lambdalabs/naruto-blip-captions
2025-11-19 11:56:00 - INFO - Dataset loaded: 1221 images
2025-11-19 11:56:00 - INFO - Caching latents for 1221 images...
2025-11-19 11:56:00 - INFO - ✓ Latent caching complete
2025-11-19 11:56:00 - INFO - ✓ Dataloader ready: 1221 training samples
2025-11-19 11:56:00 - INFO - ✓ VAE offloaded (latents cached)
2025-11-19 11:56:00 - INFO - ✓ Optimizer and scheduler configured
2025-11-19 11:56:00 - INFO - ================================================================================
2025-11-19 11:56:00 - INFO - STARTING TRAINING
2025-11-19 11:56:00 - INFO - ================================================================================
2025-11-19 11:56:00 - INFO -   Num examples: 1221
2025-11-19 11:56:00 - INFO -   Num epochs: 10
2025-11-19 11:56:00 - INFO -   Batch size per device: 1
2025-11-19 11:56:00 - INFO -   Gradient accumulation steps: 4
2025-11-19 11:56:00 - INFO -   Total optimization steps: 2000
2025-11-19 11:56:00 - INFO -   Resolution: 1024x1024
2025-11-19 11:56:00 - INFO -   LoRA rank: 16
2025-11-19 11:56:00 - INFO -   Learning rate: 0.0001
2025-11-19 11:56:00 - INFO - ================================================================================
